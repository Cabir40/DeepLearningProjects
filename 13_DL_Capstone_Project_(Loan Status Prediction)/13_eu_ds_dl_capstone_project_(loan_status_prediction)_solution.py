# -*- coding: utf-8 -*-
"""13_EU_DS_DL_Capstone_Project_(Loan_Status_Prediction)_solution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15rWD58KXtzaaG5YUddJe1moskD24wb7V

---
---

# #Tasks

#### 1. Exploratory Data Analysis

- Import Libraries, Load and Review the Data
    
#### 2. Data Cleaning

- Outlier Detection
- Handle with Missing Data

#### 3. Feature Engineering


#### 4. Data Pre-Processing

- Train-Test Split
- Scaling

 
#### 5. Model Building

- Import Libraries
- Create a Sequential Model
- Compile the Model
- Fit the Model


#### 6. Evaluating Model Performance
- Plot out the validation loss vs. the training loss
- Make Prediction
- Create classification report and confusion matrix

---
---

## 1. Exploratory Data Analysis

Get an understanding for which variables are important, view summary statistics, and visualize the data.

### Import Libraries, Load and Review the Data

***i. Import Libraries***
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# might be needed depending on your version of Jupyter
# %matplotlib inline

"""***ii. Load Data***"""

df = pd.read_csv('lending_club_loan_two.csv')
df.head()

df.info()

"""***iii. Review Data***

Some exploring/reviewing tips:

- Observe the target variable's class frequencies.
- You may be particularly interested in features that are highly correlated with the target variable. You can detect multilinearity. You can use scatter plot.
- Focus on numerical and categorical data seperately.
- Detect Number of Unique values of each column.
- Detect relationships and correlations between independent variables and target variable.
- Detect relationships and correlations between independent variables. 
- Visualize class frequencies and distributions.
- Create statistical summaries for continuous variables.

"""

sns.countplot(x='loan_status',data=df)

plt.figure(figsize=(12,4))
sns.distplot(df['loan_amnt'],kde=False,bins=40)
plt.xlim(0,45000)

plt.figure(figsize=(12,7))
sns.heatmap(df.corr(),annot=True,cmap='viridis')
#plt.ylim(10, 0)

feat_info('installment')

feat_info('loan_amnt')

sns.scatterplot(x='installment',y='loan_amnt',data=df,)

sns.boxplot(x='loan_status',y='loan_amnt',data=df)

df['grade'].unique()

feat_info('grade')

sorted(df['sub_grade'].unique())

sns.countplot(x='grade',data=df,hue='loan_status')

plt.figure(figsize=(12,4))
subgrade_order = sorted(df['sub_grade'].unique())
sns.countplot(x='sub_grade',data=df,order = subgrade_order,palette='coolwarm')

plt.figure(figsize=(12,4))
subgrade_order = sorted(df['sub_grade'].unique())
sns.countplot(x='sub_grade',data=df,order = subgrade_order,palette='coolwarm', hue='loan_status' )

"""---
---

## 2. Data Cleaning

It's a good practice to check if the data has any missing values or outliers. In real-world data, this is quite common and must be taken care of before any data pre-processing or model training.
"""

df.isnull().sum()

100* df.isnull().sum()/len(df)

feat_info('emp_title')
print('\n')
feat_info('emp_length')

df['emp_title'].nunique()

df['emp_title'].value_counts()

"""### Handle with Missing Data

There are many ways we could deal with this missing data. You could attempt to build a simple model to fill it in, such as a linear model. On the other hand you could just fill it in based on the mean of the other columns, or you could even bin the columns into categories and then set NaN as its own category.

However, you may need to take a different approach for each variable.
"""

df = df.drop('emp_title',axis=1)

sorted(df['emp_length'].dropna().unique())

emp_length_order = ['1 year',
 '10+ years',
 '2 years',
 '3 years',
 '4 years',
 '5 years',
 '6 years',
 '7 years',
 '8 years',
 '9 years',
 '< 1 year']

plt.figure(figsize=(12,4))

sns.countplot(x='emp_length',data=df,order=emp_length_order)

plt.figure(figsize=(12,4))
sns.countplot(x='emp_length',data=df,order=emp_length_order,hue='loan_status')

emp_co = df[df['loan_status']=="Charged Off"].groupby("emp_length").count()['loan_status']
emp_fp = df[df['loan_status']=="Fully Paid"].groupby("emp_length").count()['loan_status']

emp_co

emp_fp

emp_len = emp_co/emp_fp

emp_len

emp_len.plot(kind='bar')

df = df.drop('emp_length',axis=1)

df.isnull().sum()

feat_info('title')

df['title'].head(10)

df['purpose'].head(10)

df = df.drop('title',axis=1)

feat_info('mort_acc')

df['mort_acc'].value_counts()

df.corr()['mort_acc'].sort_values()

feat_info('total_acc')

df.groupby('total_acc').mean()['mort_acc']

total_acc_avg = df.groupby('total_acc').mean()['mort_acc']

total_acc_avg[3.0]

def fill_mort_acc(total_acc,mort_acc):

    if np.isnan(mort_acc):
        return total_acc_avg[total_acc]
    else:
        return mort_acc

df['mort_acc'] = df.apply(lambda x: fill_mort_acc(x['total_acc'], x['mort_acc']), axis=1)

df.isnull().sum()

df = df.dropna()

df.isnull().sum()

df.info()

df.select_dtypes(['object']).columns

df['term'].value_counts()

df['term'] = df['term'].apply(lambda term: int(term[:3]))

df.head()

df = df.drop('grade',axis=1)

subgrade_dummies = pd.get_dummies(df['sub_grade'],drop_first=True)

df = pd.concat([df.drop('sub_grade',axis=1),subgrade_dummies],axis=1)

df.select_dtypes(['object']).columns

df.head()

dummies = pd.get_dummies(df[['verification_status', 'application_type','initial_list_status','purpose' ]],drop_first=True)
df = df.drop(['verification_status', 'application_type','initial_list_status','purpose'],axis=1)
df = pd.concat([df,dummies],axis=1)

df.head()

"""## 3. Feature Engineering

Can you extract new features from existing variables?
Is there a variable that does not carry any meaningful information or causes multilinearity?
Are there variables such as timestamp that need to be converted to numeric or require a different action?
Are there any categorical variables that need to be labeled?

Below are tips for some variables. There are also different processes to do!

- Create a column called ***zip_code*** that extracts the zip code from the ***address*** column. Make this zip_code column into dummy variables using pandas
- ***issue_d*** variable would be data leakage, you wouldn't know beforehand whether or not a loan would be issued when using your model, so in theory you wouldn't have an issue_date, you can drop this feature.
- ***earliest_cr_line*** variable appears to be a historical time stamp feature. Extract the year from this feature using a .apply function, then convert it to a numeric feature. Set this new data to a feature column called ***earliest_cr_year***.Then drop the ***earliest_cr_line*** feature.
- In particular, don't forget to convert ***loan_status*** (target) to numeric form.

***Note:*** *You will need to examine all the variables one by one and manipulate most of them. A lot of work to be done!*
"""

df['home_ownership'].value_counts()

df['home_ownership']=df['home_ownership'].replace(['NONE', 'ANY'], 'OTHER')

dummies = pd.get_dummies(df['home_ownership'],drop_first=True)
df = df.drop('home_ownership',axis=1)
df = pd.concat([df,dummies],axis=1)

df.head()

df['zip_code'] = df['address'].apply(lambda address:address[-5:])

dummies = pd.get_dummies(df['zip_code'],drop_first=True)
df = df.drop(['zip_code','address'],axis=1)
df = pd.concat([df,dummies],axis=1)

df = df.drop('issue_d',axis=1)

df['earliest_cr_year'] = df['earliest_cr_line'].apply(lambda date:int(date[-4:]))
df = df.drop('earliest_cr_line',axis=1)

df.select_dtypes(['object']).columns

df.info()

"""---
---

## 4. Data Pre-Processing

### Train Test Split
You will keep some part of the data aside as a test set. The model will not use this set during training and it will be used only for checking the performance of the model is trained and un-trained states. This way, you can make sure that you are going in the right direction with your model training.
"""

df['loan_status'] = df['loan_status'].map({'Fully Paid':1,'Charged Off':0})

X = df.drop('loan_status',axis=1).values
y = df['loan_status'].values

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)





"""### Scaling 

We can make it easier for optimization algorithms to find minimas by normalizing the data before training a model.

***Note:*** *Use a MinMaxScaler to normalize the feature data X_train and X_test. Recall you don't want data leakge from the test set so you only fit on the X_train data.*
"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

X_train = scaler.fit_transform(X_train)

X_test = scaler.transform(X_test)

"""---
---

## 5. Model Building

Implement a sequential model to assess whether a new client will be able to pay back the loan. You can discover **[how to create](https://towardsdatascience.com/building-our-first-neural-network-in-keras-bdc8abbc17f5)** your first deep learning model in Python using **[Keras](https://keras.io/about/)**.

Models in Keras are defined as a sequence of layers. The Sequential model API is a way of creating deep learning models where an instance of the Sequential class is created and model layers are created and added to it. In this section, we will look at defining a simple multilayer Perceptron. You can also create a Sequential model incrementally via the add() method.

- Import libraries
- Create a Sequential model and add layers one at a time until we are happy with your network architecture. ([build model architecture - hidden layers and hidden units](https://keras.io/guides/sequential_model/))
- Compile the model by specifying an optimizer and a loss function and compute trainable parameters.
- Fit the model
- Evaluate model performance


- ***Note***:  *Use .sample() to grab a sample of the 490k+ entries to save time on training. Highly recommended for lower RAM computers or if you are not using GPU.*

### Import Libraries
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation,Dropout
from tensorflow.keras.constraints import max_norm
from tensorflow.keras.optimizers import Adam

"""### Create a Sequential Model

[How to choose the number of hidden layers and nodes?](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw)
"""



"""### Compile the Model

When compiling, you must specify some additional properties required when training the network. Remember training a network means finding the best set of weights to map inputs to outputs in your dataset.

You must specify the loss function to use to evaluate a set of weights, the optimizer is used to search through different weights for the network and any optional metrics you would like to collect and report during training.

Define the optimizer as the efficient stochastic gradient descent algorithm ***adam***. This is a popular version of gradient descent because it automatically tunes itself and gives good results in a wide range of problems.

In this case, you will use cross entropy as the loss argument. This loss is for a binary classification problems and is defined in Keras as ***binary_crossentropy***.

"""

model = Sequential()

# input layer
model.add(Dense(78,  activation='relu'))
model.add(Dropout(0.2))

# hidden layer
model.add(Dense(39, activation='relu'))
model.add(Dropout(0.2))

# hidden layer
model.add(Dense(19, activation='relu'))
model.add(Dropout(0.2))

# output layer
model.add(Dense(units=1,activation='sigmoid'))
opt = Adam(lr = 0.01)
# Compile model
model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])

"""### Fit the Model


Training occurs over epochs and each epoch is split into batches.

The training process will run for a fixed number of iterations through the dataset called epochs, that we must specify using the epochs argument. We must also set the number of dataset rows that are considered before the model weights are updated within each epoch, called the batch size and set using the batch_size argument.

Epochs and Batch_size configurations can be chosen experimentally by trial and error. We want to train the model enough so that it learns a good (or good enough) mapping of rows of input data to the output classification. The model will always have some error, but the amount of error will level out after some point for a given model configuration.

- Fit the model to the training data for at least 25 epochs. Also add in the validation data for later plotting. Optional: add in a batch_size of 256
- OPTIONAL: Save your model.
"""

model.fit(x=X_train, 
          y=y_train, 
          epochs=25,
          batch_size=256,
          validation_data=(X_test, y_test), 
          )

losses = pd.DataFrame(model.history.history)

losses[['loss','val_loss']].plot()

from sklearn.metrics import classification_report,confusion_matrix

predictions = model.predict_classes(X_test)

print(classification_report(y_test,predictions))

confusion_matrix(y_test,predictions)

"""## 6. Evaluate Model Performance

Ideally, the loss is expected to go to zero and the accuracy to 1.0 (e.g. 100%). This is not possible for any but the most trivial machine learning problems. Instead, we will always have some error in our model. The goal is to choose a model configuration and training configuration that achieve the lowest loss and highest accuracy possible for a given dataset.

- Plot out the validation loss vs. the training loss.
- Create predictions from the X_test set and display a classification report and confusion matrix for the X_test set.
- OPTIONAL: You can observe whether the model correctly predicts the loan_reload status for a randomly selected customer id.
"""

from tensorflow.keras.models import load_model

model.save('model.h5')



"""---
---

### Plot out the validation loss vs. the training loss
"""



"""### Make Prediction"""

model = load_model('/content/model.h5')

df.sample(20)

new_customer = df.drop('loan_status',axis=1).iloc[133152]
new_customer

model.predict_classes(new_customer.values.reshape(1,78))

"""### Create classification report and confusion matrix"""

# Commented out IPython magic to ensure Python compatibility.
import os
import datetime
# Load the TensorBoard notebook extension
# %load_ext tensorboard

logdir = os.path.join("logs2", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))

tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1 )

model.fit(x=X_train, 
          y=y_train, 
          epochs=25,
          batch_size=256,
          validation_data=(X_test, y_test), 
          callbacks=[tensorboard_callback]
          )

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs2